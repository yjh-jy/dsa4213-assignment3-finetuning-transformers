{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "02b601d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.viz import *\n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e46fe1",
   "metadata": {},
   "source": [
    "## Plotting Function Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a2e4f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULTS_DIR = \"results\"\n",
    "def load_all_log_histories(base_dir=RESULTS_DIR):\n",
    "    \"\"\"Walk through results folders and load log_history.json for each experiment.\"\"\"\n",
    "    all_logs = {}\n",
    "    for root, dirs, _ in os.walk(base_dir):\n",
    "        for d in dirs:\n",
    "            folder_path = os.path.join(root, d)\n",
    "            log_file = os.path.join(folder_path, \"log_history.json\")\n",
    "            if os.path.exists(log_file):\n",
    "                try:\n",
    "                    with open(log_file, \"r\") as f:\n",
    "                        log_history = json.load(f)\n",
    "                        all_logs[d] = log_history\n",
    "                except json.JSONDecodeError:\n",
    "                    print(f\"Failed to load {log_file}\")\n",
    "    return all_logs\n",
    "\n",
    "\n",
    "def load_all_metrics_histories(base_dir=RESULTS_DIR):\n",
    "    \"\"\"Walk through results folders and load metrics.json for each experiment.\"\"\"\n",
    "    all_logs = {}\n",
    "    for root, dirs, _ in os.walk(base_dir):\n",
    "        for d in dirs:\n",
    "            folder_path = os.path.join(root, d)\n",
    "            log_file = os.path.join(folder_path, \"metrics.json\")\n",
    "            if os.path.exists(log_file):\n",
    "                try:\n",
    "                    with open(log_file, \"r\") as f:\n",
    "                        log_history = json.load(f)\n",
    "                        all_logs[d] = log_history\n",
    "                except json.JSONDecodeError:\n",
    "                    print(f\"Failed to load {log_file}\")\n",
    "    return all_logs\n",
    "\n",
    "def extract_metrics(log_history):\n",
    "    \"\"\"Extract train_loss, eval_loss, eval_acc, cumulative_time, grad norms, steps.\"\"\"\n",
    "    metrics = {\n",
    "        \"train_loss\": [],\n",
    "        \"train_steps\": [],\n",
    "        \"eval_loss\": [],\n",
    "        \"eval_steps\": [],\n",
    "        \"eval_acc\": [],\n",
    "        \"eval_acc_steps\": [],\n",
    "        \"cumulative_time\": [],\n",
    "        \"grad_norms\": [],  # {module_name: list of mean abs grad per step}\n",
    "    }\n",
    "\n",
    "    cumulative_time = 0.0\n",
    "    for entry in log_history:\n",
    "        if \"loss\" in entry:\n",
    "            metrics[\"train_loss\"].append(entry[\"loss\"])\n",
    "            metrics[\"train_steps\"].append(entry.get(\"step\", len(metrics[\"train_steps\"])+1))\n",
    "        if \"eval_loss\" in entry:\n",
    "            metrics[\"eval_loss\"].append(entry[\"eval_loss\"])\n",
    "            metrics[\"eval_steps\"].append(entry.get(\"step\", len(metrics[\"eval_steps\"])+1))\n",
    "        if \"eval_accuracy\" in entry:\n",
    "            metrics[\"eval_acc\"].append(entry[\"eval_accuracy\"])\n",
    "            metrics[\"eval_acc_steps\"].append(entry.get(\"step\", len(metrics[\"eval_acc_steps\"])+1))\n",
    "        if \"time_per_step\" in entry:\n",
    "            cumulative_time += np.sum(entry[\"time_per_step\"])\n",
    "            metrics[\"cumulative_time\"].append(cumulative_time)\n",
    "        if \"grad_norm\" in entry:\n",
    "            v = entry[\"grad_norm\"]\n",
    "            metrics[\"grad_norms\"].append(v)\n",
    "    return metrics\n",
    "\n",
    "def plot_f1_per_size_curves(metrics, save_path=\"results/f1_per_size_curves.png\"):\n",
    "    \"\"\"\n",
    "    metrics: dict[strategy_name, log_history_dict or list]\n",
    "        Each log_history should contain 'dataset_size' and 'eval_f1' (scalar or list).\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(8,6), dpi=300)\n",
    "    sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "    data = []\n",
    "    for name, log_history in metrics.items():\n",
    "        # If log_history is a list of dicts (from Trainer logs)\n",
    "        if isinstance(log_history, list):\n",
    "            eval_f1s = [d.get('eval_f1') for d in log_history if 'eval_f1' in d]\n",
    "            sizes = [d.get('dataset_size') for d in log_history if 'dataset_size' in d]\n",
    "            if not sizes and 'dataset_size' in log_history[0]:\n",
    "                sizes = [log_history[0]['dataset_size']] * len(eval_f1s)\n",
    "        else:\n",
    "            eval_f1s = [log_history.get('eval_f1')]\n",
    "            sizes = [log_history.get('dataset_size')]\n",
    "\n",
    "        for s, f1 in zip(sizes, eval_f1s):\n",
    "            strat = name.split('_')[0]\n",
    "            if s is not None and f1 is not None:\n",
    "                data.append({\"strategy\": strat, \"size\": s, \"f1\": f1})\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "    sns.lineplot(data=df, x=\"size\", y=\"f1\", hue=\"strategy\", marker=\"o\", linewidth=2)\n",
    "    \n",
    "    plt.xlabel(\"Dataset Size\")\n",
    "    plt.ylabel(\"F1 Score\")\n",
    "    plt.title(\"F1 vs Dataset Size by Strategy\")\n",
    "    plt.legend(title=\"Strategy\")\n",
    "    plt.tight_layout()\n",
    "    os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "    plt.savefig(save_path, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "\n",
    "def plot_efficiency_bubble(metrics, save_path=\"results/efficiency_bubble.png\"):\n",
    "    \"\"\"\n",
    "    Bubble plot: dataset size (x) vs F1 (y), bubble size ∝ trainable parameters.\n",
    "    Expects metrics = {strategy_name: {'dataset_size': int, 'trainable_params': int, 'eval_f1': float}}.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(12, 6), dpi=300)\n",
    "    sns.set_theme(style=\"whitegrid\", context=\"talk\", palette=\"Set2\")\n",
    "    for name, log_history in metrics.items():\n",
    "        plt.scatter(\n",
    "            x=log_history['dataset_size'],\n",
    "            y=log_history['eval_f1'],\n",
    "            alpha=0.7,\n",
    "            label=name,\n",
    "            edgecolors='black',\n",
    "            linewidth=0.5\n",
    "        )\n",
    "        plt.text(\n",
    "            log_history['dataset_size'] * 1.02,\n",
    "            log_history['eval_f1'],\n",
    "            f\"{log_history['trainable_params'] // 1_000}k\",\n",
    "            fontsize=8\n",
    "        )\n",
    "\n",
    "    plt.xlabel(\"Dataset Size\")\n",
    "    plt.ylabel(\"Macro F1\")\n",
    "    plt.title(\"Compute–Performance Trade-off (Bubble ∝ Trainable Parameters)\")\n",
    "    plt.legend(title=\"Strategy\", bbox_to_anchor=(1.05, 1), loc=\"upper left\")\n",
    "    plt.tight_layout()\n",
    "\n",
    "    os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "    plt.savefig(save_path, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "\n",
    "def plot_loss_curves(all_logs, normalize=True, save_path=\"results/plots/loss_curves.png\"):\n",
    "    plt.figure(figsize=(20,13), dpi=300)\n",
    "    sns.set_palette(\"Paired\")\n",
    "    for name, log_history in all_logs.items():\n",
    "        m = extract_metrics(log_history)\n",
    "        x_train = np.array(m[\"train_steps\"])\n",
    "        x_eval = np.array(m[\"eval_steps\"])\n",
    "        x_train_norm = x_train / max(x_train)\n",
    "        x_eval_norm = x_eval / max(x_train)\n",
    "        sns.lineplot(x=x_train_norm, y=m[\"train_loss\"], label=f\"{name} train\", linewidth=2)\n",
    "        sns.lineplot(x=x_eval_norm, y=m[\"eval_loss\"], label=f\"{name} eval\", linewidth=2, linestyle=\"--\")\n",
    "    plt.xlabel(\"Normalized Training Progress\" if normalize else \"Step\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Training and Evaluation Loss\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig(save_path, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "\n",
    "def plot_accuracy_curves(all_logs, normalize=True, save_path=\"results/plots/accuracy_curves.png\"):\n",
    "    plt.figure(figsize=(12,8), dpi=300)\n",
    "    for name, log_history in all_logs.items():\n",
    "        m = extract_metrics(log_history)\n",
    "        x_eval = np.array(m[\"eval_acc_steps\"])\n",
    "        if normalize and len(x_eval) > 0:\n",
    "            x_eval = x_eval / max(x_eval)\n",
    "        if len(m[\"eval_acc\"]) > 0:\n",
    "            plt.plot(x_eval, m[\"eval_acc\"], label=name, linewidth=2)\n",
    "    plt.xlabel(\"Normalized Training Progress\" if normalize else \"Step\")\n",
    "    plt.ylabel(\"Eval Accuracy\")\n",
    "    plt.title(\"Evaluation Accuracy\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig(save_path, bbox_inches=\"tight\")\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef7a696",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List, Any\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "def build_summary_table(results_data: Dict[str, List[Dict[str,Any]]],\n",
    "                        trainable_params_map: Dict[str,int]=None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    results_data: dict name -> list of dicts (log_history)\n",
    "    trainable_params_map: optional mapping name->int used when log_history doesn't contain trainable_params\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    for name, history in results_data.items():\n",
    "        trainable_params = None\n",
    "        eval_f1 = max([entry['eval_f1'] for entry in history if 'eval_f1' in entry]) \n",
    "        total_flos = max([entry['total_flos'] for entry in history if 'total_flos' in entry])\n",
    "        train_time = max([entry['train_runtime'] for entry in history if 'train_runtime' in entry])\n",
    "        \n",
    "        # fallback: try to infer dataset size or strategy from name e.g., 'lora_size25000'\n",
    "        dataset_size = None\n",
    "        parts = name.split(\"_\")\n",
    "        for p in parts:\n",
    "            if p.startswith(\"size\"):\n",
    "                try:\n",
    "                    dataset_size = int(p.replace(\"size\",\"\"))\n",
    "                except:\n",
    "                    pass\n",
    "        # try provided mapping for trainable params\n",
    "        if (trainable_params is None) and trainable_params_map and name in trainable_params_map:\n",
    "            trainable_params = int(trainable_params_map[name])\n",
    "\n",
    "        row = {\n",
    "            \"name\": name,\n",
    "            \"strategy\": parts[0] if parts else name,\n",
    "            \"dataset_size\": dataset_size,\n",
    "            \"trainable_params\": trainable_params,\n",
    "            \"eval_f1\": eval_f1,\n",
    "            \"total_flos\": total_flos,\n",
    "            \"train_time\": train_time\n",
    "        }\n",
    "        rows.append(row)\n",
    "    df = pd.DataFrame(rows)\n",
    "    return df\n",
    "\n",
    "def compute_f1_petaflop(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    df[\"petaflops\"] = df[\"total_flos\"] / 1e15\n",
    "    # avoid divide-by-zero or NaN\n",
    "    df[\"f1_per_petaflop\"] = df.apply(\n",
    "        lambda r: (r[\"eval_f1\"] / r[\"petaflops\"]) if pd.notna(r[\"eval_f1\"]) and pd.notna(r[\"petaflops\"]) and r[\"petaflops\"]>0 else None,\n",
    "        axis=1\n",
    "    )\n",
    "    # optionally f1_per_hour if train_time available\n",
    "    df[\"f1_per_hour\"] = df.apply(\n",
    "        lambda r: (r[\"eval_f1\"] / (r[\"train_time\"]/3600.0)) if pd.notna(r[\"eval_f1\"]) and pd.notna(r[\"train_time\"]) and r[\"train_time\"]>0 else None,\n",
    "        axis=1\n",
    "    )\n",
    "    df[\"f1_per_mcompute\"] = df.apply(\n",
    "        lambda r: (r[\"eval_f1\"] / (r[\"trainable_params\"]/ 1e6)) if pd.notna(r[\"eval_f1\"]) and pd.notna(r[\"trainable_params\"]) and r[\"trainable_params\"]>0 else None,\n",
    "        axis=1\n",
    "    )\n",
    "    return df\n",
    "\n",
    "def plot_f1_per_petaflop(df: pd.DataFrame, out_path=os.path.join('results/plots',\"f1_per_petaflop_simple.png\")):\n",
    "    dfp = df.dropna(subset=[\"f1_per_petaflop\"]).copy()\n",
    "    if dfp.empty:\n",
    "        print(\"No data to plot F1 per PetaFLOP.\")\n",
    "        return\n",
    "    dfp = dfp.sort_values(\"f1_per_petaflop\", ascending=False)\n",
    "    plt.figure(figsize=(10,5), dpi=150)\n",
    "    ax = sns.barplot(data=dfp, x=\"name\", y=\"f1_per_petaflop\", hue=\"strategy\")\n",
    "    ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha=\"right\")\n",
    "    ax.set_ylabel(\"Macro F1 per PetaFLOP\")\n",
    "    ax.set_title(\"Macro F1 per PetaFLOP\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_path, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "    print(\"Saved\", out_path)\n",
    "\n",
    "def plot_f1_per_mparams(df: pd.DataFrame, out_path=os.path.join('results/plots',\"f1_per_mparams.png\")):\n",
    "    dfp = df.dropna(subset=[\"f1_per_mcompute\"]).copy()\n",
    "    if dfp.empty:\n",
    "        print(\"No data to plot F1 per mparams.\")\n",
    "        return\n",
    "    dfp = dfp.sort_values(\"f1_per_mcompute\", ascending=False)\n",
    "    plt.figure(figsize=(10,5), dpi=300)\n",
    "    ax = sns.barplot(data=dfp, x=\"name\", y=\"f1_per_mcompute\", hue=\"strategy\")\n",
    "    ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha=\"right\")\n",
    "    ax.set_ylabel(\"Macro F1 per Million Params\")\n",
    "    ax.set_title(\"Parameter Effiency\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_path, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "    print(\"Saved\", out_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef671cc",
   "metadata": {},
   "source": [
    "## Produce Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af9ae449",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             name strategy  dataset_size  trainable_params   eval_f1  \\\n",
      "0  full_size10000     full         10000          67700000  0.904591   \n",
      "1  full_size25000     full         25000          67700000  0.916356   \n",
      "2   full_size5000     full          5000          67700000  0.894634   \n",
      "3  lora_size10000     lora         10000            739586  0.882311   \n",
      "4  lora_size25000     lora         25000            739586  0.896599   \n",
      "5   lora_size5000     lora          5000            739586  0.868918   \n",
      "\n",
      "     total_flos  f1_per_petaflop  f1_per_hour  f1_per_mcompute  \n",
      "0  1.987011e+15         0.455252     1.544346         0.013362  \n",
      "1  4.967527e+15         0.184469     0.967879         0.013536  \n",
      "2  9.935055e+14         0.900482     1.933205         0.013215  \n",
      "3  2.021091e+15         0.436552     1.548938         1.192980  \n",
      "4  5.052728e+15         0.177449     1.037948         1.212299  \n",
      "5  1.010546e+15         0.859850     1.850905         1.174871  \n",
      "Saved results/plots/f1_per_mparams.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l6/t004tdcn2gj42jvly6ft8z500000gn/T/ipykernel_52629/4046942619.py:89: UserWarning: set_ticklabels() should only be used with a fixed number of ticks, i.e. after set_ticks() or using a FixedLocator.\n",
      "  ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha=\"right\")\n"
     ]
    }
   ],
   "source": [
    "all_logs = load_all_log_histories()\n",
    "\n",
    "# optional map if trainable_params missing in logs\n",
    "trainable_params_map = {\n",
    "    \"full_size10000\": 67700000,\n",
    "    \"full_size25000\": 67700000,\n",
    "    \"full_size5000\": 67700000,\n",
    "    \"lora_size10000\": 739586,\n",
    "    \"lora_size25000\": 739586,\n",
    "    \"lora_size5000\": 739586\n",
    "}\n",
    "\n",
    "df = build_summary_table(all_logs, trainable_params_map)\n",
    "df = compute_f1_petaflop(df)\n",
    "print(df[[\"name\",\"strategy\",\"dataset_size\",\"trainable_params\",\"eval_f1\",\"total_flos\",\"f1_per_petaflop\",\"f1_per_hour\",\"f1_per_mcompute\"]])\n",
    "plot_f1_per_mparams(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ead26814",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'full_size10000': {'strategy': 'full', 'dataset_size': 10000, 'trainable_params': 66955010, 'train_time': 2109.243427991867, 'train_runtime_from_trainer': 2108.6784, 'train_samples': 10000, 'eval_loss': 0.25121867656707764, 'eval_accuracy': 0.9046, 'eval_f1': 0.904591280253631, 'eval_steps_used': 156, 'logging_steps_used': 39}, 'full_size25000': {'strategy': 'full', 'dataset_size': 25000, 'trainable_params': 66955010, 'train_time': 3408.66047000885, 'train_runtime_from_trainer': 3408.3631, 'train_samples': 25000, 'eval_loss': 0.3417150676250458, 'eval_accuracy': 0.91636, 'eval_f1': 0.916356267615916, 'eval_steps_used': 390, 'logging_steps_used': 97}, 'full_size5000': {'strategy': 'full', 'dataset_size': 5000, 'trainable_params': 66955010, 'train_time': 1666.4126119613647, 'train_runtime_from_trainer': 1665.9797, 'train_samples': 5000, 'eval_loss': 0.37619733810424805, 'eval_accuracy': 0.89464, 'eval_f1': 0.8946335235863073, 'eval_steps_used': 78, 'logging_steps_used': 19}, 'lora_size10000': {'strategy': 'lora', 'dataset_size': 10000, 'trainable_params': 739586, 'train_time': 2050.7333691120148, 'train_runtime_from_trainer': 2050.6443, 'train_samples': 10000, 'eval_loss': 0.27724799513816833, 'eval_accuracy': 0.88236, 'eval_f1': 0.8823112145148283, 'eval_steps_used': 156, 'logging_steps_used': 39}, 'lora_size25000': {'strategy': 'lora', 'dataset_size': 25000, 'trainable_params': 739586, 'train_time': 3109.839716911316, 'train_runtime_from_trainer': 3109.7498, 'train_samples': 25000, 'eval_loss': 0.24904228746891022, 'eval_accuracy': 0.8966, 'eval_f1': 0.8965994241001525, 'eval_steps_used': 390, 'logging_steps_used': 97}, 'lora_size5000': {'strategy': 'lora', 'dataset_size': 5000, 'trainable_params': 739586, 'train_time': 1690.135426044464, 'train_runtime_from_trainer': 1690.0405, 'train_samples': 5000, 'eval_loss': 0.3037601709365845, 'eval_accuracy': 0.86892, 'eval_f1': 0.8689179444236372, 'eval_steps_used': 78, 'logging_steps_used': 19}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l6/t004tdcn2gj42jvly6ft8z500000gn/T/ipykernel_42154/2835005580.py:131: UserWarning: Glyph 8733 (\\N{PROPORTIONAL TO}) missing from font(s) Arial.\n",
      "  plt.tight_layout()\n",
      "/var/folders/l6/t004tdcn2gj42jvly6ft8z500000gn/T/ipykernel_42154/2835005580.py:134: UserWarning: Glyph 8733 (\\N{PROPORTIONAL TO}) missing from font(s) Arial.\n",
      "  plt.savefig(save_path, bbox_inches=\"tight\")\n"
     ]
    }
   ],
   "source": [
    "results = load_all_metrics_histories('results')\n",
    "print(results)\n",
    "# Make F1 vs size plot\n",
    "os.makedirs(os.path.join('results', \"plots\"), exist_ok=True)\n",
    "plot_f1_per_size_curves(results, save_path=os.path.join('results', \"plots\", \"acc_vs_size.png\"))\n",
    "\n",
    "# Make efficiency tradeoff plot\n",
    "plot_efficiency_bubble(results, save_path=os.path.join('results', \"plots\", \"efficiency_tradeoff.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4cb8c412",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'full_size10000': [{'loss': 0.5034, 'grad_norm': 5.185615539550781, 'learning_rate': 4.79765708200213e-05, 'epoch': 0.12460063897763578, 'step': 39}, {'loss': 0.3352, 'grad_norm': 2.747762680053711, 'learning_rate': 4.589989350372737e-05, 'epoch': 0.24920127795527156, 'step': 78}, {'loss': 0.3127, 'grad_norm': 7.535058975219727, 'learning_rate': 4.3823216187433444e-05, 'epoch': 0.3738019169329074, 'step': 117}, {'loss': 0.3382, 'grad_norm': 2.5630247592926025, 'learning_rate': 4.174653887113951e-05, 'epoch': 0.4984025559105431, 'step': 156}, {'eval_loss': 0.28028789162635803, 'eval_accuracy': 0.89412, 'eval_f1': 0.8941009847728258, 'eval_runtime': 200.8735, 'eval_samples_per_second': 124.456, 'eval_steps_per_second': 3.893, 'epoch': 0.4984025559105431, 'step': 156}, {'loss': 0.2866, 'grad_norm': 2.7168471813201904, 'learning_rate': 3.966986155484558e-05, 'epoch': 0.6230031948881789, 'step': 195}, {'loss': 0.2887, 'grad_norm': 3.5869998931884766, 'learning_rate': 3.759318423855165e-05, 'epoch': 0.7476038338658147, 'step': 234}, {'loss': 0.2669, 'grad_norm': 5.375754356384277, 'learning_rate': 3.551650692225773e-05, 'epoch': 0.8722044728434505, 'step': 273}, {'loss': 0.2775, 'grad_norm': 2.418363094329834, 'learning_rate': 3.343982960596379e-05, 'epoch': 0.9968051118210862, 'step': 312}, {'eval_loss': 0.25736328959465027, 'eval_accuracy': 0.89444, 'eval_f1': 0.8942650231624667, 'eval_runtime': 201.7475, 'eval_samples_per_second': 123.917, 'eval_steps_per_second': 3.876, 'epoch': 0.9968051118210862, 'step': 312}, {'loss': 0.1631, 'grad_norm': 4.3292717933654785, 'learning_rate': 3.136315228966986e-05, 'epoch': 1.121405750798722, 'step': 351}, {'loss': 0.1447, 'grad_norm': 7.458074569702148, 'learning_rate': 2.928647497337593e-05, 'epoch': 1.2460063897763578, 'step': 390}, {'loss': 0.1736, 'grad_norm': 4.04138708114624, 'learning_rate': 2.7209797657082004e-05, 'epoch': 1.3706070287539935, 'step': 429}, {'loss': 0.165, 'grad_norm': 4.054087162017822, 'learning_rate': 2.5133120340788073e-05, 'epoch': 1.4952076677316293, 'step': 468}, {'eval_loss': 0.25121867656707764, 'eval_accuracy': 0.9046, 'eval_f1': 0.904591280253631, 'eval_runtime': 203.3797, 'eval_samples_per_second': 122.923, 'eval_steps_per_second': 3.845, 'epoch': 1.4952076677316293, 'step': 468}, {'loss': 0.1635, 'grad_norm': 7.487450122833252, 'learning_rate': 2.3056443024494142e-05, 'epoch': 1.619808306709265, 'step': 507}, {'loss': 0.1563, 'grad_norm': 2.5266082286834717, 'learning_rate': 2.0979765708200215e-05, 'epoch': 1.744408945686901, 'step': 546}, {'loss': 0.1618, 'grad_norm': 8.157320022583008, 'learning_rate': 1.8903088391906284e-05, 'epoch': 1.8690095846645367, 'step': 585}, {'loss': 0.1597, 'grad_norm': 7.128623962402344, 'learning_rate': 1.6826411075612357e-05, 'epoch': 1.9936102236421727, 'step': 624}, {'eval_loss': 0.2863175868988037, 'eval_accuracy': 0.90252, 'eval_f1': 0.9024095618196695, 'eval_runtime': 201.5736, 'eval_samples_per_second': 124.024, 'eval_steps_per_second': 3.879, 'epoch': 1.9936102236421727, 'step': 624}, {'loss': 0.0596, 'grad_norm': 0.3402571678161621, 'learning_rate': 1.4749733759318424e-05, 'epoch': 2.1182108626198084, 'step': 663}, {'loss': 0.0689, 'grad_norm': 7.173252105712891, 'learning_rate': 1.2673056443024495e-05, 'epoch': 2.242811501597444, 'step': 702}, {'loss': 0.05, 'grad_norm': 0.2687002718448639, 'learning_rate': 1.0596379126730564e-05, 'epoch': 2.36741214057508, 'step': 741}, {'loss': 0.05, 'grad_norm': 0.13068123161792755, 'learning_rate': 8.519701810436635e-06, 'epoch': 2.4920127795527156, 'step': 780}, {'eval_loss': 0.3875455856323242, 'eval_accuracy': 0.90132, 'eval_f1': 0.9012840628184464, 'eval_runtime': 202.9109, 'eval_samples_per_second': 123.207, 'eval_steps_per_second': 3.854, 'epoch': 2.4920127795527156, 'step': 780}, {'loss': 0.0625, 'grad_norm': 1.0424216985702515, 'learning_rate': 6.443024494142705e-06, 'epoch': 2.6166134185303513, 'step': 819}, {'loss': 0.0711, 'grad_norm': 7.779958248138428, 'learning_rate': 4.366347177848776e-06, 'epoch': 2.741214057507987, 'step': 858}, {'loss': 0.0802, 'grad_norm': 1.243196964263916, 'learning_rate': 2.2896698615548457e-06, 'epoch': 2.8658146964856233, 'step': 897}, {'loss': 0.0811, 'grad_norm': 2.341804265975952, 'learning_rate': 2.129925452609159e-07, 'epoch': 2.9904153354632586, 'step': 936}, {'eval_loss': 0.37954339385032654, 'eval_accuracy': 0.90164, 'eval_f1': 0.9016379186583589, 'eval_runtime': 202.1418, 'eval_samples_per_second': 123.676, 'eval_steps_per_second': 3.869, 'epoch': 2.9904153354632586, 'step': 936}, {'train_runtime': 2108.6784, 'train_samples_per_second': 14.227, 'train_steps_per_second': 0.445, 'total_flos': 1987010979840000.0, 'train_loss': 0.18372599726604324, 'epoch': 3.0, 'step': 939}], 'full_size25000': [{'loss': 0.3917, 'grad_norm': 4.810973644256592, 'learning_rate': 4.7953964194373404e-05, 'epoch': 0.12404092071611253, 'step': 97}, {'loss': 0.3073, 'grad_norm': 4.350192070007324, 'learning_rate': 4.588661551577153e-05, 'epoch': 0.24808184143222506, 'step': 194}, {'loss': 0.28, 'grad_norm': 8.38672924041748, 'learning_rate': 4.381926683716965e-05, 'epoch': 0.3721227621483376, 'step': 291}, {'loss': 0.2827, 'grad_norm': 5.877252578735352, 'learning_rate': 4.175191815856778e-05, 'epoch': 0.4961636828644501, 'step': 388}, {'eval_loss': 0.23529468476772308, 'eval_accuracy': 0.9032, 'eval_f1': 0.9031998953010067, 'eval_runtime': 201.1378, 'eval_samples_per_second': 124.293, 'eval_steps_per_second': 3.888, 'epoch': 0.49872122762148335, 'step': 390}, {'loss': 0.2587, 'grad_norm': 4.467191219329834, 'learning_rate': 3.96845694799659e-05, 'epoch': 0.6202046035805626, 'step': 485}, {'loss': 0.2759, 'grad_norm': 6.2456769943237305, 'learning_rate': 3.7617220801364025e-05, 'epoch': 0.7442455242966752, 'step': 582}, {'loss': 0.2622, 'grad_norm': 2.937839984893799, 'learning_rate': 3.554987212276215e-05, 'epoch': 0.8682864450127877, 'step': 679}, {'loss': 0.2464, 'grad_norm': 4.712180137634277, 'learning_rate': 3.348252344416027e-05, 'epoch': 0.9923273657289002, 'step': 776}, {'eval_loss': 0.2713564932346344, 'eval_accuracy': 0.89872, 'eval_f1': 0.8983560306725467, 'eval_runtime': 201.4723, 'eval_samples_per_second': 124.087, 'eval_steps_per_second': 3.881, 'epoch': 0.9974424552429667, 'step': 780}, {'loss': 0.1402, 'grad_norm': 7.298550605773926, 'learning_rate': 3.14151747655584e-05, 'epoch': 1.1163682864450128, 'step': 873}, {'loss': 0.1524, 'grad_norm': 6.648622989654541, 'learning_rate': 2.9347826086956526e-05, 'epoch': 1.2404092071611252, 'step': 970}, {'loss': 0.1541, 'grad_norm': 6.217440605163574, 'learning_rate': 2.728047740835465e-05, 'epoch': 1.3644501278772379, 'step': 1067}, {'loss': 0.1259, 'grad_norm': 6.777977466583252, 'learning_rate': 2.5213128729752773e-05, 'epoch': 1.4884910485933505, 'step': 1164}, {'eval_loss': 0.28005361557006836, 'eval_accuracy': 0.90424, 'eval_f1': 0.9041695172165635, 'eval_runtime': 201.6034, 'eval_samples_per_second': 124.006, 'eval_steps_per_second': 3.879, 'epoch': 1.49616368286445, 'step': 1170}, {'loss': 0.1493, 'grad_norm': 4.240861415863037, 'learning_rate': 2.3145780051150897e-05, 'epoch': 1.612531969309463, 'step': 1261}, {'loss': 0.1571, 'grad_norm': 6.165780067443848, 'learning_rate': 2.107843137254902e-05, 'epoch': 1.7365728900255755, 'step': 1358}, {'loss': 0.1421, 'grad_norm': 6.648932456970215, 'learning_rate': 1.9011082693947144e-05, 'epoch': 1.8606138107416879, 'step': 1455}, {'loss': 0.1366, 'grad_norm': 5.299874782562256, 'learning_rate': 1.694373401534527e-05, 'epoch': 1.9846547314578005, 'step': 1552}, {'eval_loss': 0.23021742701530457, 'eval_accuracy': 0.91392, 'eval_f1': 0.913907767464611, 'eval_runtime': 201.5585, 'eval_samples_per_second': 124.033, 'eval_steps_per_second': 3.88, 'epoch': 1.9948849104859336, 'step': 1560}, {'loss': 0.0696, 'grad_norm': 7.00210428237915, 'learning_rate': 1.4876385336743392e-05, 'epoch': 2.108695652173913, 'step': 1649}, {'loss': 0.0637, 'grad_norm': 9.617870330810547, 'learning_rate': 1.2809036658141517e-05, 'epoch': 2.2327365728900257, 'step': 1746}, {'loss': 0.0669, 'grad_norm': 2.1011269092559814, 'learning_rate': 1.0741687979539643e-05, 'epoch': 2.3567774936061383, 'step': 1843}, {'loss': 0.0724, 'grad_norm': 4.451705455780029, 'learning_rate': 8.674339300937768e-06, 'epoch': 2.4808184143222505, 'step': 1940}, {'eval_loss': 0.30587294697761536, 'eval_accuracy': 0.91548, 'eval_f1': 0.9154798527320953, 'eval_runtime': 201.6058, 'eval_samples_per_second': 124.004, 'eval_steps_per_second': 3.879, 'epoch': 2.493606138107417, 'step': 1950}, {'loss': 0.0593, 'grad_norm': 6.630263805389404, 'learning_rate': 6.6069906223358914e-06, 'epoch': 2.604859335038363, 'step': 2037}, {'loss': 0.0569, 'grad_norm': 1.0553652048110962, 'learning_rate': 4.539641943734015e-06, 'epoch': 2.7289002557544757, 'step': 2134}, {'loss': 0.0542, 'grad_norm': 1.7883689403533936, 'learning_rate': 2.47229326513214e-06, 'epoch': 2.8529411764705883, 'step': 2231}, {'loss': 0.056, 'grad_norm': 0.2582383453845978, 'learning_rate': 4.0494458653026426e-07, 'epoch': 2.976982097186701, 'step': 2328}, {'eval_loss': 0.3417150676250458, 'eval_accuracy': 0.91636, 'eval_f1': 0.916356267615916, 'eval_runtime': 203.2776, 'eval_samples_per_second': 122.984, 'eval_steps_per_second': 3.847, 'epoch': 2.9923273657289, 'step': 2340}, {'train_runtime': 3408.3631, 'train_samples_per_second': 22.005, 'train_steps_per_second': 0.688, 'total_flos': 4967527449600000.0, 'train_loss': 0.16397292365757501, 'epoch': 3.0, 'step': 2346}], 'full_size5000': [{'loss': 0.638, 'grad_norm': 5.508829593658447, 'learning_rate': 4.8089171974522296e-05, 'epoch': 0.12101910828025478, 'step': 19}, {'loss': 0.4127, 'grad_norm': 6.567084312438965, 'learning_rate': 4.607218683651805e-05, 'epoch': 0.24203821656050956, 'step': 38}, {'loss': 0.3355, 'grad_norm': 5.286606788635254, 'learning_rate': 4.4055201698513805e-05, 'epoch': 0.3630573248407643, 'step': 57}, {'loss': 0.3199, 'grad_norm': 2.3652546405792236, 'learning_rate': 4.2038216560509556e-05, 'epoch': 0.4840764331210191, 'step': 76}, {'eval_loss': 0.29253268241882324, 'eval_accuracy': 0.87848, 'eval_f1': 0.8784580454391966, 'eval_runtime': 202.3942, 'eval_samples_per_second': 123.521, 'eval_steps_per_second': 3.864, 'epoch': 0.4968152866242038, 'step': 78}, {'loss': 0.3577, 'grad_norm': 3.150437116622925, 'learning_rate': 4.002123142250531e-05, 'epoch': 0.6050955414012739, 'step': 95}, {'loss': 0.295, 'grad_norm': 10.180156707763672, 'learning_rate': 3.8004246284501064e-05, 'epoch': 0.7261146496815286, 'step': 114}, {'loss': 0.3255, 'grad_norm': 5.27089262008667, 'learning_rate': 3.5987261146496815e-05, 'epoch': 0.8471337579617835, 'step': 133}, {'loss': 0.3279, 'grad_norm': 4.38800048828125, 'learning_rate': 3.3970276008492566e-05, 'epoch': 0.9681528662420382, 'step': 152}, {'eval_loss': 0.4219159781932831, 'eval_accuracy': 0.8278, 'eval_f1': 0.8237295916516527, 'eval_runtime': 201.8814, 'eval_samples_per_second': 123.835, 'eval_steps_per_second': 3.874, 'epoch': 0.9936305732484076, 'step': 156}, {'loss': 0.2357, 'grad_norm': 4.245553016662598, 'learning_rate': 3.1953290870488323e-05, 'epoch': 1.089171974522293, 'step': 171}, {'loss': 0.1404, 'grad_norm': 10.38921070098877, 'learning_rate': 2.9936305732484078e-05, 'epoch': 1.2101910828025477, 'step': 190}, {'loss': 0.2055, 'grad_norm': 9.820645332336426, 'learning_rate': 2.7919320594479832e-05, 'epoch': 1.3312101910828025, 'step': 209}, {'loss': 0.2211, 'grad_norm': 5.16783332824707, 'learning_rate': 2.5902335456475586e-05, 'epoch': 1.4522292993630574, 'step': 228}, {'eval_loss': 0.27172040939331055, 'eval_accuracy': 0.89092, 'eval_f1': 0.8907975938872832, 'eval_runtime': 203.2938, 'eval_samples_per_second': 122.975, 'eval_steps_per_second': 3.847, 'epoch': 1.4904458598726116, 'step': 234}, {'loss': 0.1846, 'grad_norm': 6.069005012512207, 'learning_rate': 2.388535031847134e-05, 'epoch': 1.573248407643312, 'step': 247}, {'loss': 0.1574, 'grad_norm': 8.022784233093262, 'learning_rate': 2.1868365180467095e-05, 'epoch': 1.694267515923567, 'step': 266}, {'loss': 0.1669, 'grad_norm': 4.044009208679199, 'learning_rate': 1.9851380042462846e-05, 'epoch': 1.8152866242038217, 'step': 285}, {'loss': 0.1537, 'grad_norm': 5.440385818481445, 'learning_rate': 1.78343949044586e-05, 'epoch': 1.9363057324840764, 'step': 304}, {'eval_loss': 0.31445372104644775, 'eval_accuracy': 0.88648, 'eval_f1': 0.8861778807651649, 'eval_runtime': 202.8801, 'eval_samples_per_second': 123.225, 'eval_steps_per_second': 3.854, 'epoch': 1.9872611464968153, 'step': 312}, {'loss': 0.1591, 'grad_norm': 2.1627256870269775, 'learning_rate': 1.5817409766454354e-05, 'epoch': 2.0573248407643314, 'step': 323}, {'loss': 0.0836, 'grad_norm': 5.663701057434082, 'learning_rate': 1.3800424628450107e-05, 'epoch': 2.178343949044586, 'step': 342}, {'loss': 0.0784, 'grad_norm': 1.250835657119751, 'learning_rate': 1.178343949044586e-05, 'epoch': 2.299363057324841, 'step': 361}, {'loss': 0.0638, 'grad_norm': 1.1490797996520996, 'learning_rate': 9.766454352441615e-06, 'epoch': 2.4203821656050954, 'step': 380}, {'eval_loss': 0.37440210580825806, 'eval_accuracy': 0.89456, 'eval_f1': 0.8945528403909357, 'eval_runtime': 202.2826, 'eval_samples_per_second': 123.59, 'eval_steps_per_second': 3.866, 'epoch': 2.484076433121019, 'step': 390}, {'loss': 0.0778, 'grad_norm': 3.6912739276885986, 'learning_rate': 7.749469214437368e-06, 'epoch': 2.5414012738853504, 'step': 399}, {'loss': 0.0643, 'grad_norm': 0.8862868547439575, 'learning_rate': 5.732484076433121e-06, 'epoch': 2.662420382165605, 'step': 418}, {'loss': 0.1158, 'grad_norm': 0.8240408301353455, 'learning_rate': 3.715498938428875e-06, 'epoch': 2.78343949044586, 'step': 437}, {'loss': 0.0752, 'grad_norm': 8.3051118850708, 'learning_rate': 1.6985138004246284e-06, 'epoch': 2.904458598726115, 'step': 456}, {'eval_loss': 0.37619733810424805, 'eval_accuracy': 0.89464, 'eval_f1': 0.8946335235863073, 'eval_runtime': 201.8911, 'eval_samples_per_second': 123.829, 'eval_steps_per_second': 3.873, 'epoch': 2.980891719745223, 'step': 468}, {'train_runtime': 1665.9797, 'train_samples_per_second': 9.004, 'train_steps_per_second': 0.283, 'total_flos': 993505489920000.0, 'train_loss': 0.2124819694810612, 'epoch': 3.0, 'step': 471}], 'lora_size10000': [{'loss': 0.6619, 'grad_norm': 0.9652343988418579, 'learning_rate': 9.59531416400426e-05, 'epoch': 0.12460063897763578, 'step': 39}, {'loss': 0.4866, 'grad_norm': 3.0220913887023926, 'learning_rate': 9.179978700745474e-05, 'epoch': 0.24920127795527156, 'step': 78}, {'loss': 0.3538, 'grad_norm': 3.2168374061584473, 'learning_rate': 8.764643237486689e-05, 'epoch': 0.3738019169329074, 'step': 117}, {'loss': 0.3658, 'grad_norm': 1.0145472288131714, 'learning_rate': 8.349307774227903e-05, 'epoch': 0.4984025559105431, 'step': 156}, {'eval_loss': 0.32641392946243286, 'eval_accuracy': 0.85836, 'eval_f1': 0.8583194387821789, 'eval_runtime': 220.9367, 'eval_samples_per_second': 113.155, 'eval_steps_per_second': 3.539, 'epoch': 0.4984025559105431, 'step': 156}, {'loss': 0.3374, 'grad_norm': 0.9174788594245911, 'learning_rate': 7.933972310969116e-05, 'epoch': 0.6230031948881789, 'step': 195}, {'loss': 0.3356, 'grad_norm': 1.2863489389419556, 'learning_rate': 7.51863684771033e-05, 'epoch': 0.7476038338658147, 'step': 234}, {'loss': 0.3251, 'grad_norm': 1.6173005104064941, 'learning_rate': 7.103301384451545e-05, 'epoch': 0.8722044728434505, 'step': 273}, {'loss': 0.3038, 'grad_norm': 1.611777663230896, 'learning_rate': 6.687965921192758e-05, 'epoch': 0.9968051118210862, 'step': 312}, {'eval_loss': 0.30327749252319336, 'eval_accuracy': 0.87008, 'eval_f1': 0.870034064839877, 'eval_runtime': 220.2724, 'eval_samples_per_second': 113.496, 'eval_steps_per_second': 3.55, 'epoch': 0.9968051118210862, 'step': 312}, {'loss': 0.3037, 'grad_norm': 2.0107762813568115, 'learning_rate': 6.272630457933972e-05, 'epoch': 1.121405750798722, 'step': 351}, {'loss': 0.3037, 'grad_norm': 2.1033082008361816, 'learning_rate': 5.857294994675186e-05, 'epoch': 1.2460063897763578, 'step': 390}, {'loss': 0.3133, 'grad_norm': 2.0811398029327393, 'learning_rate': 5.441959531416401e-05, 'epoch': 1.3706070287539935, 'step': 429}, {'loss': 0.3091, 'grad_norm': 2.4505183696746826, 'learning_rate': 5.0266240681576146e-05, 'epoch': 1.4952076677316293, 'step': 468}, {'eval_loss': 0.28262051939964294, 'eval_accuracy': 0.87804, 'eval_f1': 0.8780342937612542, 'eval_runtime': 220.7921, 'eval_samples_per_second': 113.229, 'eval_steps_per_second': 3.542, 'epoch': 1.4952076677316293, 'step': 468}, {'loss': 0.2928, 'grad_norm': 3.7765612602233887, 'learning_rate': 4.6112886048988285e-05, 'epoch': 1.619808306709265, 'step': 507}, {'loss': 0.2812, 'grad_norm': 1.020183801651001, 'learning_rate': 4.195953141640043e-05, 'epoch': 1.744408945686901, 'step': 546}, {'loss': 0.2791, 'grad_norm': 3.303114175796509, 'learning_rate': 3.780617678381257e-05, 'epoch': 1.8690095846645367, 'step': 585}, {'loss': 0.2941, 'grad_norm': 3.0515947341918945, 'learning_rate': 3.365282215122471e-05, 'epoch': 1.9936102236421727, 'step': 624}, {'eval_loss': 0.27792778611183167, 'eval_accuracy': 0.88084, 'eval_f1': 0.8808382060906897, 'eval_runtime': 220.2605, 'eval_samples_per_second': 113.502, 'eval_steps_per_second': 3.55, 'epoch': 1.9936102236421727, 'step': 624}, {'loss': 0.2791, 'grad_norm': 3.069120407104492, 'learning_rate': 2.9499467518636848e-05, 'epoch': 2.1182108626198084, 'step': 663}, {'loss': 0.2783, 'grad_norm': 2.6075448989868164, 'learning_rate': 2.534611288604899e-05, 'epoch': 2.242811501597444, 'step': 702}, {'loss': 0.2906, 'grad_norm': 2.3023860454559326, 'learning_rate': 2.1192758253461128e-05, 'epoch': 2.36741214057508, 'step': 741}, {'loss': 0.2405, 'grad_norm': 1.9780267477035522, 'learning_rate': 1.703940362087327e-05, 'epoch': 2.4920127795527156, 'step': 780}, {'eval_loss': 0.28129324316978455, 'eval_accuracy': 0.88156, 'eval_f1': 0.8815276676683945, 'eval_runtime': 220.2189, 'eval_samples_per_second': 113.523, 'eval_steps_per_second': 3.551, 'epoch': 2.4920127795527156, 'step': 780}, {'loss': 0.2954, 'grad_norm': 2.0890393257141113, 'learning_rate': 1.288604898828541e-05, 'epoch': 2.6166134185303513, 'step': 819}, {'loss': 0.2746, 'grad_norm': 1.4711253643035889, 'learning_rate': 8.732694355697551e-06, 'epoch': 2.741214057507987, 'step': 858}, {'loss': 0.2822, 'grad_norm': 1.0063117742538452, 'learning_rate': 4.579339723109691e-06, 'epoch': 2.8658146964856233, 'step': 897}, {'loss': 0.301, 'grad_norm': 1.5319818258285522, 'learning_rate': 4.259850905218318e-07, 'epoch': 2.9904153354632586, 'step': 936}, {'eval_loss': 0.27724799513816833, 'eval_accuracy': 0.88236, 'eval_f1': 0.8823112145148283, 'eval_runtime': 219.1893, 'eval_samples_per_second': 114.057, 'eval_steps_per_second': 3.568, 'epoch': 2.9904153354632586, 'step': 936}, {'train_runtime': 2050.6443, 'train_samples_per_second': 14.63, 'train_steps_per_second': 0.458, 'total_flos': 2021091102720000.0, 'train_loss': 0.3240301687750446, 'epoch': 3.0, 'step': 939}], 'lora_size25000': [{'loss': 0.5371, 'grad_norm': 5.836539268493652, 'learning_rate': 9.590792838874681e-05, 'epoch': 0.12404092071611253, 'step': 97}, {'loss': 0.3355, 'grad_norm': 0.9422958493232727, 'learning_rate': 9.177323103154306e-05, 'epoch': 0.24808184143222506, 'step': 194}, {'loss': 0.3021, 'grad_norm': 1.0551471710205078, 'learning_rate': 8.76385336743393e-05, 'epoch': 0.3721227621483376, 'step': 291}, {'loss': 0.3254, 'grad_norm': 1.071453332901001, 'learning_rate': 8.350383631713556e-05, 'epoch': 0.4961636828644501, 'step': 388}, {'eval_loss': 0.287322461605072, 'eval_accuracy': 0.87676, 'eval_f1': 0.8767001056460388, 'eval_runtime': 220.4024, 'eval_samples_per_second': 113.429, 'eval_steps_per_second': 3.548, 'epoch': 0.49872122762148335, 'step': 390}, {'loss': 0.2883, 'grad_norm': 1.1848031282424927, 'learning_rate': 7.93691389599318e-05, 'epoch': 0.6202046035805626, 'step': 485}, {'loss': 0.2972, 'grad_norm': 2.8746066093444824, 'learning_rate': 7.523444160272805e-05, 'epoch': 0.7442455242966752, 'step': 582}, {'loss': 0.2923, 'grad_norm': 2.6954383850097656, 'learning_rate': 7.10997442455243e-05, 'epoch': 0.8682864450127877, 'step': 679}, {'loss': 0.2827, 'grad_norm': 2.3410756587982178, 'learning_rate': 6.696504688832054e-05, 'epoch': 0.9923273657289002, 'step': 776}, {'eval_loss': 0.2722935080528259, 'eval_accuracy': 0.88812, 'eval_f1': 0.8880289238015424, 'eval_runtime': 219.6204, 'eval_samples_per_second': 113.833, 'eval_steps_per_second': 3.561, 'epoch': 0.9974424552429667, 'step': 780}, {'loss': 0.2673, 'grad_norm': 2.3371524810791016, 'learning_rate': 6.28303495311168e-05, 'epoch': 1.1163682864450128, 'step': 873}, {'loss': 0.2719, 'grad_norm': 2.12963604927063, 'learning_rate': 5.869565217391305e-05, 'epoch': 1.2404092071611252, 'step': 970}, {'loss': 0.26, 'grad_norm': 1.8476794958114624, 'learning_rate': 5.45609548167093e-05, 'epoch': 1.3644501278772379, 'step': 1067}, {'loss': 0.2571, 'grad_norm': 4.222049713134766, 'learning_rate': 5.0426257459505546e-05, 'epoch': 1.4884910485933505, 'step': 1164}, {'eval_loss': 0.2608475089073181, 'eval_accuracy': 0.8914, 'eval_f1': 0.8913809593428585, 'eval_runtime': 219.4031, 'eval_samples_per_second': 113.946, 'eval_steps_per_second': 3.564, 'epoch': 1.49616368286445, 'step': 1170}, {'loss': 0.2647, 'grad_norm': 1.8811225891113281, 'learning_rate': 4.629156010230179e-05, 'epoch': 1.612531969309463, 'step': 1261}, {'loss': 0.2896, 'grad_norm': 1.2156305313110352, 'learning_rate': 4.215686274509804e-05, 'epoch': 1.7365728900255755, 'step': 1358}, {'loss': 0.2486, 'grad_norm': 2.79414439201355, 'learning_rate': 3.802216538789429e-05, 'epoch': 1.8606138107416879, 'step': 1455}, {'loss': 0.2642, 'grad_norm': 1.5487549304962158, 'learning_rate': 3.388746803069054e-05, 'epoch': 1.9846547314578005, 'step': 1552}, {'eval_loss': 0.25360107421875, 'eval_accuracy': 0.89396, 'eval_f1': 0.8939124997885654, 'eval_runtime': 219.021, 'eval_samples_per_second': 114.144, 'eval_steps_per_second': 3.57, 'epoch': 1.9948849104859336, 'step': 1560}, {'loss': 0.2583, 'grad_norm': 1.9011188745498657, 'learning_rate': 2.9752770673486785e-05, 'epoch': 2.108695652173913, 'step': 1649}, {'loss': 0.2674, 'grad_norm': 1.321955680847168, 'learning_rate': 2.5618073316283035e-05, 'epoch': 2.2327365728900257, 'step': 1746}, {'loss': 0.2457, 'grad_norm': 2.960625171661377, 'learning_rate': 2.1483375959079285e-05, 'epoch': 2.3567774936061383, 'step': 1843}, {'loss': 0.2472, 'grad_norm': 5.27838659286499, 'learning_rate': 1.7348678601875536e-05, 'epoch': 2.4808184143222505, 'step': 1940}, {'eval_loss': 0.24904228746891022, 'eval_accuracy': 0.8966, 'eval_f1': 0.8965994241001525, 'eval_runtime': 218.9221, 'eval_samples_per_second': 114.196, 'eval_steps_per_second': 3.572, 'epoch': 2.493606138107417, 'step': 1950}, {'loss': 0.2555, 'grad_norm': 1.1696933507919312, 'learning_rate': 1.3213981244671783e-05, 'epoch': 2.604859335038363, 'step': 2037}, {'loss': 0.2536, 'grad_norm': 2.9245107173919678, 'learning_rate': 9.07928388746803e-06, 'epoch': 2.7289002557544757, 'step': 2134}, {'loss': 0.2436, 'grad_norm': 2.1543631553649902, 'learning_rate': 4.94458653026428e-06, 'epoch': 2.8529411764705883, 'step': 2231}, {'loss': 0.2551, 'grad_norm': 1.2390928268432617, 'learning_rate': 8.098891730605285e-07, 'epoch': 2.976982097186701, 'step': 2328}, {'eval_loss': 0.24923965334892273, 'eval_accuracy': 0.89576, 'eval_f1': 0.8957487241820075, 'eval_runtime': 220.1633, 'eval_samples_per_second': 113.552, 'eval_steps_per_second': 3.552, 'epoch': 2.9923273657289, 'step': 2340}, {'train_runtime': 3109.7498, 'train_samples_per_second': 24.118, 'train_steps_per_second': 0.754, 'total_flos': 5052727756800000.0, 'train_loss': 0.28347206562884525, 'epoch': 3.0, 'step': 2346}], 'lora_size5000': [{'loss': 0.687, 'grad_norm': 0.5003830194473267, 'learning_rate': 9.617834394904459e-05, 'epoch': 0.12101910828025478, 'step': 19}, {'loss': 0.6555, 'grad_norm': 0.6071833372116089, 'learning_rate': 9.21443736730361e-05, 'epoch': 0.24203821656050956, 'step': 38}, {'loss': 0.6081, 'grad_norm': 0.780384361743927, 'learning_rate': 8.811040339702761e-05, 'epoch': 0.3630573248407643, 'step': 57}, {'loss': 0.5002, 'grad_norm': 1.3705358505249023, 'learning_rate': 8.407643312101911e-05, 'epoch': 0.4840764331210191, 'step': 76}, {'eval_loss': 0.40141385793685913, 'eval_accuracy': 0.82776, 'eval_f1': 0.8264058926364131, 'eval_runtime': 220.0882, 'eval_samples_per_second': 113.591, 'eval_steps_per_second': 3.553, 'epoch': 0.4968152866242038, 'step': 78}, {'loss': 0.4276, 'grad_norm': 3.285250663757324, 'learning_rate': 8.004246284501063e-05, 'epoch': 0.6050955414012739, 'step': 95}, {'loss': 0.3573, 'grad_norm': 3.815969228744507, 'learning_rate': 7.600849256900213e-05, 'epoch': 0.7261146496815286, 'step': 114}, {'loss': 0.3807, 'grad_norm': 1.6452194452285767, 'learning_rate': 7.197452229299363e-05, 'epoch': 0.8471337579617835, 'step': 133}, {'loss': 0.3481, 'grad_norm': 1.113581657409668, 'learning_rate': 6.794055201698513e-05, 'epoch': 0.9681528662420382, 'step': 152}, {'eval_loss': 0.332390159368515, 'eval_accuracy': 0.85368, 'eval_f1': 0.8536773694702774, 'eval_runtime': 219.3754, 'eval_samples_per_second': 113.96, 'eval_steps_per_second': 3.565, 'epoch': 0.9936305732484076, 'step': 156}, {'loss': 0.3448, 'grad_norm': 1.0189634561538696, 'learning_rate': 6.390658174097665e-05, 'epoch': 1.089171974522293, 'step': 171}, {'loss': 0.3048, 'grad_norm': 1.7131977081298828, 'learning_rate': 5.9872611464968155e-05, 'epoch': 1.2101910828025477, 'step': 190}, {'loss': 0.3326, 'grad_norm': 1.2544177770614624, 'learning_rate': 5.5838641188959664e-05, 'epoch': 1.3312101910828025, 'step': 209}, {'loss': 0.3544, 'grad_norm': 2.0744106769561768, 'learning_rate': 5.180467091295117e-05, 'epoch': 1.4522292993630574, 'step': 228}, {'eval_loss': 0.31778237223625183, 'eval_accuracy': 0.86072, 'eval_f1': 0.8606787832112253, 'eval_runtime': 221.0287, 'eval_samples_per_second': 113.107, 'eval_steps_per_second': 3.538, 'epoch': 1.4904458598726116, 'step': 234}, {'loss': 0.3415, 'grad_norm': 1.1422151327133179, 'learning_rate': 4.777070063694268e-05, 'epoch': 1.573248407643312, 'step': 247}, {'loss': 0.3241, 'grad_norm': 2.559316396713257, 'learning_rate': 4.373673036093419e-05, 'epoch': 1.694267515923567, 'step': 266}, {'loss': 0.3382, 'grad_norm': 1.088457465171814, 'learning_rate': 3.970276008492569e-05, 'epoch': 1.8152866242038217, 'step': 285}, {'loss': 0.3327, 'grad_norm': 0.7919942736625671, 'learning_rate': 3.56687898089172e-05, 'epoch': 1.9363057324840764, 'step': 304}, {'eval_loss': 0.3081493377685547, 'eval_accuracy': 0.866, 'eval_f1': 0.8659809577167505, 'eval_runtime': 219.1627, 'eval_samples_per_second': 114.071, 'eval_steps_per_second': 3.568, 'epoch': 1.9872611464968153, 'step': 312}, {'loss': 0.2986, 'grad_norm': 1.4412823915481567, 'learning_rate': 3.163481953290871e-05, 'epoch': 2.0573248407643314, 'step': 323}, {'loss': 0.3195, 'grad_norm': 3.1606931686401367, 'learning_rate': 2.7600849256900213e-05, 'epoch': 2.178343949044586, 'step': 342}, {'loss': 0.3058, 'grad_norm': 2.2425460815429688, 'learning_rate': 2.356687898089172e-05, 'epoch': 2.299363057324841, 'step': 361}, {'loss': 0.2994, 'grad_norm': 1.4718962907791138, 'learning_rate': 1.953290870488323e-05, 'epoch': 2.4203821656050954, 'step': 380}, {'eval_loss': 0.30705854296684265, 'eval_accuracy': 0.86772, 'eval_f1': 0.8676933216813838, 'eval_runtime': 221.0004, 'eval_samples_per_second': 113.122, 'eval_steps_per_second': 3.538, 'epoch': 2.484076433121019, 'step': 390}, {'loss': 0.3093, 'grad_norm': 2.173198938369751, 'learning_rate': 1.5498938428874735e-05, 'epoch': 2.5414012738853504, 'step': 399}, {'loss': 0.2683, 'grad_norm': 0.7609800100326538, 'learning_rate': 1.1464968152866242e-05, 'epoch': 2.662420382165605, 'step': 418}, {'loss': 0.3319, 'grad_norm': 2.295454740524292, 'learning_rate': 7.43099787685775e-06, 'epoch': 2.78343949044586, 'step': 437}, {'loss': 0.3131, 'grad_norm': 1.5254709720611572, 'learning_rate': 3.397027600849257e-06, 'epoch': 2.904458598726115, 'step': 456}, {'eval_loss': 0.3037601709365845, 'eval_accuracy': 0.86892, 'eval_f1': 0.8689179444236372, 'eval_runtime': 220.492, 'eval_samples_per_second': 113.383, 'eval_steps_per_second': 3.547, 'epoch': 2.980891719745223, 'step': 468}, {'train_runtime': 1690.0405, 'train_samples_per_second': 8.876, 'train_steps_per_second': 0.279, 'total_flos': 1010545551360000.0, 'train_loss': 0.3755081241550972, 'epoch': 3.0, 'step': 471}]}\n"
     ]
    }
   ],
   "source": [
    "all_logs = load_all_log_histories()\n",
    "print(all_logs)\n",
    "plot_loss_curves(all_logs)\n",
    "plot_accuracy_curves(all_logs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
