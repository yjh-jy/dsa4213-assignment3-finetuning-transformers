{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "02b601d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.viz import *\n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e46fe1",
   "metadata": {},
   "source": [
    "## Plotting Function Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3a2e4f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULTS_DIR = \"results\"\n",
    "def load_all_log_histories(base_dir=RESULTS_DIR):\n",
    "    \"\"\"Walk through results folders and load log_history.json for each experiment.\"\"\"\n",
    "    all_logs = {}\n",
    "    for root, dirs, _ in os.walk(base_dir):\n",
    "        for d in dirs:\n",
    "            folder_path = os.path.join(root, d)\n",
    "            log_file = os.path.join(folder_path, \"log_history.json\")\n",
    "            if os.path.exists(log_file):\n",
    "                try:\n",
    "                    with open(log_file, \"r\") as f:\n",
    "                        log_history = json.load(f)\n",
    "                        all_logs[d] = log_history\n",
    "                except json.JSONDecodeError:\n",
    "                    print(f\"Failed to load {log_file}\")\n",
    "    return all_logs\n",
    "\n",
    "\n",
    "def load_all_metrics_histories(base_dir=RESULTS_DIR):\n",
    "    \"\"\"Walk through results folders and load metrics.json for each experiment.\"\"\"\n",
    "    all_logs = {}\n",
    "    for root, dirs, _ in os.walk(base_dir):\n",
    "        for d in dirs:\n",
    "            folder_path = os.path.join(root, d)\n",
    "            log_file = os.path.join(folder_path, \"metrics.json\")\n",
    "            if os.path.exists(log_file):\n",
    "                try:\n",
    "                    with open(log_file, \"r\") as f:\n",
    "                        log_history = json.load(f)\n",
    "                        all_logs[d] = log_history\n",
    "                except json.JSONDecodeError:\n",
    "                    print(f\"Failed to load {log_file}\")\n",
    "    return all_logs\n",
    "\n",
    "def extract_metrics(log_history):\n",
    "    \"\"\"Extract train_loss, eval_loss, eval_acc, cumulative_time, grad norms, steps.\"\"\"\n",
    "    metrics = {\n",
    "        \"train_loss\": [],\n",
    "        \"train_steps\": [],\n",
    "        \"eval_loss\": [],\n",
    "        \"eval_steps\": [],\n",
    "        \"eval_acc\": [],\n",
    "        \"eval_acc_steps\": [],\n",
    "        \"cumulative_time\": [],\n",
    "        \"grad_norms\": [],  # {module_name: list of mean abs grad per step}\n",
    "    }\n",
    "\n",
    "    cumulative_time = 0.0\n",
    "    for entry in log_history:\n",
    "        if \"loss\" in entry:\n",
    "            metrics[\"train_loss\"].append(entry[\"loss\"])\n",
    "            metrics[\"train_steps\"].append(entry.get(\"step\", len(metrics[\"train_steps\"])+1))\n",
    "        if \"eval_loss\" in entry:\n",
    "            metrics[\"eval_loss\"].append(entry[\"eval_loss\"])\n",
    "            metrics[\"eval_steps\"].append(entry.get(\"step\", len(metrics[\"eval_steps\"])+1))\n",
    "        if \"eval_accuracy\" in entry:\n",
    "            metrics[\"eval_acc\"].append(entry[\"eval_accuracy\"])\n",
    "            metrics[\"eval_acc_steps\"].append(entry.get(\"step\", len(metrics[\"eval_acc_steps\"])+1))\n",
    "        if \"time_per_step\" in entry:\n",
    "            cumulative_time += np.sum(entry[\"time_per_step\"])\n",
    "            metrics[\"cumulative_time\"].append(cumulative_time)\n",
    "        if \"grad_norm\" in entry:\n",
    "            v = entry[\"grad_norm\"]\n",
    "            metrics[\"grad_norms\"].append(v)\n",
    "    return metrics\n",
    "\n",
    "def plot_f1_per_size_curves(metrics, save_path=\"results/f1_per_size_curves.png\"):\n",
    "    \"\"\"\n",
    "    metrics: dict[strategy_name, log_history_dict or list]\n",
    "        Each log_history should contain 'dataset_size' and 'eval_f1' (scalar or list).\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(8,6), dpi=300)\n",
    "    sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "    data = []\n",
    "    for name, log_history in metrics.items():\n",
    "        # If log_history is a list of dicts (from Trainer logs)\n",
    "        if isinstance(log_history, list):\n",
    "            eval_f1s = [d.get('eval_f1') for d in log_history if 'eval_f1' in d]\n",
    "            sizes = [d.get('dataset_size') for d in log_history if 'dataset_size' in d]\n",
    "            if not sizes and 'dataset_size' in log_history[0]:\n",
    "                sizes = [log_history[0]['dataset_size']] * len(eval_f1s)\n",
    "        else:\n",
    "            eval_f1s = [log_history.get('eval_f1')]\n",
    "            sizes = [log_history.get('dataset_size')]\n",
    "\n",
    "        for s, f1 in zip(sizes, eval_f1s):\n",
    "            strat = name.split('_')[0]\n",
    "            if s is not None and f1 is not None:\n",
    "                data.append({\"strategy\": strat, \"size\": s, \"f1\": f1})\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "    sns.lineplot(data=df, x=\"size\", y=\"f1\", hue=\"strategy\", marker=\"o\", linewidth=2)\n",
    "    \n",
    "    plt.xlabel(\"Dataset Size\")\n",
    "    plt.ylabel(\"F1 Score\")\n",
    "    plt.title(\"F1 vs Dataset Size by Strategy\")\n",
    "    plt.legend(title=\"Strategy\")\n",
    "    plt.tight_layout()\n",
    "    os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "    plt.savefig(save_path, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "\n",
    "def plot_efficiency_bubble(metrics, save_path=\"results/efficiency_bubble.png\"):\n",
    "    \"\"\"\n",
    "    Bubble plot: dataset size (x) vs F1 (y), bubble size ∝ trainable parameters.\n",
    "    Expects metrics = {strategy_name: {'dataset_size': int, 'trainable_params': int, 'eval_f1': float}}.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(12, 6), dpi=300)\n",
    "    sns.set_theme(style=\"whitegrid\", context=\"talk\", palette=\"Set2\")\n",
    "    for name, log_history in metrics.items():\n",
    "        plt.scatter(\n",
    "            x=log_history['dataset_size'],\n",
    "            y=log_history['eval_f1'],\n",
    "            alpha=0.7,\n",
    "            label=name,\n",
    "            edgecolors='black',\n",
    "            linewidth=0.5\n",
    "        )\n",
    "        plt.text(\n",
    "            log_history['dataset_size'] * 1.02,\n",
    "            log_history['eval_f1'],\n",
    "            f\"{log_history['trainable_params'] // 1_000}k\",\n",
    "            fontsize=8\n",
    "        )\n",
    "\n",
    "    plt.xlabel(\"Dataset Size\")\n",
    "    plt.ylabel(\"Macro F1\")\n",
    "    plt.title(\"Compute–Performance Trade-off (Bubble ∝ Trainable Parameters)\")\n",
    "    plt.legend(title=\"Strategy\", bbox_to_anchor=(1.05, 1), loc=\"upper left\")\n",
    "    plt.tight_layout()\n",
    "\n",
    "    os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "    plt.savefig(save_path, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "\n",
    "def plot_loss_curves(all_logs, normalize=True, save_path=\"results/plots/loss_curves.png\"):\n",
    "    plt.figure(figsize=(20,13), dpi=300)\n",
    "    sns.set_palette(\"Paired\")\n",
    "    for name, log_history in all_logs.items():\n",
    "        m = extract_metrics(log_history)\n",
    "        x_train = np.array(m[\"train_steps\"])\n",
    "        x_eval = np.array(m[\"eval_steps\"])\n",
    "        x_train_norm = x_train / max(x_train)\n",
    "        x_eval_norm = x_eval / max(x_train)\n",
    "        sns.lineplot(x=x_train_norm, y=m[\"train_loss\"], label=f\"{name} train\", linewidth=2)\n",
    "        sns.lineplot(x=x_eval_norm, y=m[\"eval_loss\"], label=f\"{name} eval\", linewidth=2, linestyle=\"--\")\n",
    "    plt.xlabel(\"Normalized Training Progress\" if normalize else \"Step\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Training and Evaluation Loss\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig(save_path, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "\n",
    "def plot_accuracy_curves(all_logs, normalize=True, save_path=\"results/plots/accuracy_curves.png\"):\n",
    "    plt.figure(figsize=(12,8), dpi=300)\n",
    "    for name, log_history in all_logs.items():\n",
    "        m = extract_metrics(log_history)\n",
    "        x_eval = np.array(m[\"eval_acc_steps\"])\n",
    "        if normalize and len(x_eval) > 0:\n",
    "            x_eval = x_eval / max(x_eval)\n",
    "        if len(m[\"eval_acc\"]) > 0:\n",
    "            plt.plot(x_eval, m[\"eval_acc\"], label=name, linewidth=2)\n",
    "    plt.xlabel(\"Normalized Training Progress\" if normalize else \"Step\")\n",
    "    plt.ylabel(\"Eval Accuracy\")\n",
    "    plt.title(\"Evaluation Accuracy\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig(save_path, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "\n",
    "def plot_cumulative_time(all_logs, normalize=True, save_path=\"results/time_curves.png\"):\n",
    "    plt.figure(figsize=(10,6))\n",
    "    for name, log_history in all_logs.items():\n",
    "        m = extract_metrics(log_history)\n",
    "        x_train = np.array(m[\"train_steps\"])\n",
    "        y_time = np.array(m[\"cumulative_time\"])\n",
    "        if normalize and len(x_train) > 0:\n",
    "            x_train = x_train / max(x_train)\n",
    "        plt.plot(x_train, y_time, label=name, linewidth=2)\n",
    "    plt.xlabel(\"Normalized Training Progress\" if normalize else \"Step\")\n",
    "    plt.ylabel(\"Cumulative Training Time (s)\")\n",
    "    plt.title(\"Cumulative Training Time\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig(save_path, bbox_inches=\"tight\")\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef671cc",
   "metadata": {},
   "source": [
    "## Produce Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ead26814",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'full_size10000': {'strategy': 'full', 'dataset_size': 10000, 'trainable_params': 66955010, 'train_time': 2109.243427991867, 'train_runtime_from_trainer': 2108.6784, 'train_samples': 10000, 'eval_loss': 0.25121867656707764, 'eval_accuracy': 0.9046, 'eval_f1': 0.904591280253631, 'eval_steps_used': 156, 'logging_steps_used': 39}, 'full_size25000': {'strategy': 'full', 'dataset_size': 25000, 'trainable_params': 66955010, 'train_time': 3408.66047000885, 'train_runtime_from_trainer': 3408.3631, 'train_samples': 25000, 'eval_loss': 0.3417150676250458, 'eval_accuracy': 0.91636, 'eval_f1': 0.916356267615916, 'eval_steps_used': 390, 'logging_steps_used': 97}, 'full_size5000': {'strategy': 'full', 'dataset_size': 5000, 'trainable_params': 66955010, 'train_time': 1666.4126119613647, 'train_runtime_from_trainer': 1665.9797, 'train_samples': 5000, 'eval_loss': 0.37619733810424805, 'eval_accuracy': 0.89464, 'eval_f1': 0.8946335235863073, 'eval_steps_used': 78, 'logging_steps_used': 19}, 'lora_size10000': {'strategy': 'lora', 'dataset_size': 10000, 'trainable_params': 739586, 'train_time': 2050.7333691120148, 'train_runtime_from_trainer': 2050.6443, 'train_samples': 10000, 'eval_loss': 0.27724799513816833, 'eval_accuracy': 0.88236, 'eval_f1': 0.8823112145148283, 'eval_steps_used': 156, 'logging_steps_used': 39}, 'lora_size25000': {'strategy': 'lora', 'dataset_size': 25000, 'trainable_params': 739586, 'train_time': 3109.839716911316, 'train_runtime_from_trainer': 3109.7498, 'train_samples': 25000, 'eval_loss': 0.24904228746891022, 'eval_accuracy': 0.8966, 'eval_f1': 0.8965994241001525, 'eval_steps_used': 390, 'logging_steps_used': 97}, 'lora_size5000': {'strategy': 'lora', 'dataset_size': 5000, 'trainable_params': 739586, 'train_time': 1690.135426044464, 'train_runtime_from_trainer': 1690.0405, 'train_samples': 5000, 'eval_loss': 0.3037601709365845, 'eval_accuracy': 0.86892, 'eval_f1': 0.8689179444236372, 'eval_steps_used': 78, 'logging_steps_used': 19}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l6/t004tdcn2gj42jvly6ft8z500000gn/T/ipykernel_42154/2835005580.py:131: UserWarning: Glyph 8733 (\\N{PROPORTIONAL TO}) missing from font(s) Arial.\n",
      "  plt.tight_layout()\n",
      "/var/folders/l6/t004tdcn2gj42jvly6ft8z500000gn/T/ipykernel_42154/2835005580.py:134: UserWarning: Glyph 8733 (\\N{PROPORTIONAL TO}) missing from font(s) Arial.\n",
      "  plt.savefig(save_path, bbox_inches=\"tight\")\n"
     ]
    }
   ],
   "source": [
    "results = load_all_metrics_histories('results')\n",
    "print(results)\n",
    "# Make F1 vs size plot\n",
    "os.makedirs(os.path.join('results', \"plots\"), exist_ok=True)\n",
    "plot_f1_per_size_curves(results, save_path=os.path.join('results', \"plots\", \"acc_vs_size.png\"))\n",
    "\n",
    "# Make efficiency tradeoff plot\n",
    "plot_efficiency_bubble(results, save_path=os.path.join('results', \"plots\", \"efficiency_tradeoff.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4cb8c412",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_logs = load_all_log_histories()\n",
    "plot_loss_curves(all_logs)\n",
    "plot_accuracy_curves(all_logs)\n",
    "# plot_cumulative_time(all_logs)\n",
    "# plot_gradients(all_logs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
